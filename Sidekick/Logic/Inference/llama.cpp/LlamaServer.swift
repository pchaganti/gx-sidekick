//
//  LlamaServer.swift
//  Sidekick
//
//  Created by Bean John on 10/9/24.
//

import EventSource
import Foundation
import FSKit_macOS
import OSLog
import SimilaritySearchKit

/// The inference server where LLM inference happens
public actor LlamaServer {
    
    // MARK: - Core Configuration
    
    static let logger: Logger = .init(
        subsystem: Bundle.main.bundleIdentifier!,
        category: String(describing: LlamaServer.self)
    )
    
    var modelType: ModelType
    var systemPrompt: String
    var contextLength: Int
    
    // MARK: - Connection Details
    
    var host: String = "127.0.0.1"
    var port: String
    var scheme: String = "http"
    
    // MARK: - Process State
    
    var isStartingServer: Bool = false
    var isCancelled: Bool = false
    
    var eventSource: EventSource?
    var dataTask: EventSource.DataTask?
    
    var monitor: Process = Process()
    var process: Process = Process()
    var session: URLSession?
    
    // MARK: - Initialization
    
    init(
        modelType: ModelType,
        systemPrompt: String = InferenceSettings.systemPrompt,
        contextLength: Int = InferenceSettings.contextLength
    ) {
        self.modelType = modelType
        self.systemPrompt = systemPrompt
        self.contextLength = contextLength
        self.port = {
            switch modelType {
                case .regular:
                    return "4579"
                case .worker:
                    return "9830"
                case .completions:
                    return "1623"
            }
        }()
    }
    
    // MARK: - Model Metadata
    
    var modelUrl: URL? {
        switch self.modelType {
            case .regular:
                return Settings.modelUrl
            case .worker:
                return InferenceSettings.workerModelUrl
            case .completions:
                return InferenceSettings.completionsModelUrl
        }
    }
    
    var modelName: String {
        return self.modelUrl?.deletingPathExtension().lastPathComponent ?? "Unknown Model"
    }
    
    func modelHasVision(
        usingRemoteModel: Bool
    ) -> Bool {
        return Self.modelHasVision(
            type: self.modelType,
            usingRemoteModel: usingRemoteModel
        )
    }
    
    static func modelHasVision(
        type: ModelType,
        usingRemoteModel: Bool
    ) -> Bool {
        // Return false if using non-regular model
        if type != .regular {
            return false
        }
        // Using local model and has vision enabled, and has projector
        if !usingRemoteModel && InferenceSettings.localModelHasVision {
            return true
        }
        print("\(InferenceSettings.serverModelName) vision: \(InferenceSettings.serverModelHasVision)")
        // Else, get toggle value
        return InferenceSettings.serverModelHasVision
    }
    
    // MARK: - Configuration Mutations
    
    public func setSystemPrompt(_ systemPrompt: String) {
        self.systemPrompt = systemPrompt
    }
    
    /// Function executed when output finishes
    /// - Parameter text: The output generated by the LLM
    public func onFinish(text: String) {}
    
}
